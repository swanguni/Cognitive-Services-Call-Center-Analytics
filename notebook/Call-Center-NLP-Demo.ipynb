{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Center Analysis using Jupyter Notebook\n",
    "\n",
    "This notebook demonstrates the entire data flow and tasks to analyze Customer Audio files using Microsoft Azure Cognitive services, Azure Blob Store and Azure SQL Data Warehouse. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import urllib.request, json\n",
    "import csv\n",
    "import uuid\n",
    "import operator\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import pyodbc\n",
    "import os\n",
    "import io\n",
    "#import tables\n",
    "import time\n",
    "import azure\n",
    "#from azure.storage.blob import BlockBlobService\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "import logging\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finding(object):\n",
    "    cx_id = \"\"\n",
    "    cx_feedback_theme = \"\"\n",
    "    cx_intent = \"\"\n",
    "    cx_keywords =\"\"\n",
    "    cx_channel_id =\"\"\n",
    "    cx_named_entity=\"\"\n",
    "    cx_transcript_offset=\"\"\n",
    "    cx_raw_transcript=\"\"\n",
    "    cx_transcript_confidence=\"\"\n",
    "    cx_sentiment=\"\"\n",
    "    \n",
    "    # The class \"constructor\" - It's actually an initializer \n",
    "    def __init__(self, id, offset):\n",
    "        self.cx_id = id\n",
    "        self.cx_transcript_offset=offset\n",
    "\n",
    "    def set_cx_raw_transcript(self, cx_raw_transcript):\n",
    "        self.cx_raw_transcript=cx_raw_transcript\n",
    "\n",
    "    def set_cx_transcript_confidence(self, cx_transcript_confidence):\n",
    "        self.cx_transcript_confidence=cx_transcript_confidence\n",
    "\n",
    "    def set_cx_sentiment(self, cx_transcript_offset):\n",
    "        self.cx_transcript_offset=cx_transcript_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG,format=\"%(asctime)s %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p %Z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP using Azure Cognitive Service "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech To Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serviceHostname = \"https://eastus.api.cognitive.microsoft.com\"\n",
    "service_region = \"eastus\"\n",
    "name = \"Call Center Conversation Transcription\"\n",
    "description = \"Call Center Conversation Sppech to Text Transcription Demo\"\n",
    "locale = \"en-US\"\n",
    "\n",
    "# replace with the Azure Cognitive Speech Key\n",
    "sub_key_speech = \"\"\n",
    "\n",
    "#identifier for transption finding\n",
    "stt_findings_id=\"123456789findings1\"\n",
    "\n",
    "### Replace with Call Center Recordings Samples\n",
    "recordings_blob_uri_inbound_1 =\"\"\n",
    "recordings_blob_uri_inbound_2 = \"\"\n",
    "recordings_blob_uri_outbound_1 = \"\"\n",
    "recordings_blob_uri_outbound_2 = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Ocp-Apim-Subscription-Key': sub_key_speech,\n",
    "        }\n",
    "\n",
    "body = {\n",
    "        'contentUrls': [ recordings_blob_uri_inbound_1, \n",
    "                         recordings_blob_uri_inbound_2,\n",
    "                         recordings_blob_uri_outbound_1,\n",
    "                         recordings_blob_uri_outbound_2\n",
    "        ],\n",
    "        'properties': {\n",
    "                      },\n",
    "        'locale': locale,\n",
    "        'displayName': name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe():\n",
    "    logging.info(\"Starting call center batch transcriptions...\")\n",
    "    segment_results = []\n",
    "    \n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('eastus.api.cognitive.microsoft.com')\n",
    "        conn.request(\"POST\", \"/speechtotext/v3.0/transcriptions\", body=json.dumps(body), headers=headers)\n",
    "        data = conn.getresponse().read().decode('utf-8')\n",
    "        json_obj = json.loads(data)\n",
    "        transcriptions_id = json_obj['links'].get('files').replace(\"/files\", \"\").replace(\"https://eastus.api.cognitive.microsoft.com/speechtotext/v3.0/transcriptions/\", \"\")\n",
    "        \n",
    "        # Log information about the created transcription. If you should ask for support, please\n",
    "        # include this information.\n",
    "        logging.info(f\"Created new transcription with id '{transcriptions_id}' in region {service_region}\")\n",
    "        logging.info(\"Checking status .....\")\n",
    "\n",
    "        completed = False\n",
    "        while not completed:\n",
    "            # wait for 60 seconds before refreshing the transcription status\n",
    "            time.sleep(60)\n",
    "            \n",
    "            conn.request(\"GET\", f\"/speechtotext/v3.0/transcriptions/{transcriptions_id}\", body=json.dumps({}), headers=headers)\n",
    "            data = conn.getresponse().read().decode('utf-8')\n",
    "            #print(data)\n",
    "            json_obj = json.loads(data)\n",
    "            status = json_obj['status']\n",
    "            logging.info(f\"Transcriptions status: {status}\")\n",
    "\n",
    "            if status in (\"Failed\", \"Succeeded\"):\n",
    "                completed = True\n",
    "\n",
    "            if status == \"Succeeded\":\n",
    "                \n",
    "                conn = http.client.HTTPSConnection('eastus.api.cognitive.microsoft.com')\n",
    "                conn.request(\"GET\", f\"/speechtotext/v3.0/transcriptions/{transcriptions_id}/files\", body=json.dumps({}), headers=headers)\n",
    "                response = conn.getresponse()\n",
    "                data = response.read().decode('utf-8')\n",
    "                json_obj = json.loads(data)\n",
    "\n",
    "                for value in json_obj['values']:\n",
    "                    if value.get('kind') == \"Transcription\":\n",
    "                        results_url = value.get('links').get('contentUrl')\n",
    "                        results = requests.get(results_url).content.decode('utf-8')\n",
    "                        logging.info(f\"Results for {results_url}:\\n{results}\")\n",
    "                        segment_results.append(results)\n",
    "               \n",
    "            elif status == \"Failed\":\n",
    "                logging.info(\"Transcription failed .......\")\n",
    "    \n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    return segment_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_results = transcribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings = []\n",
    "\n",
    "for segment in segment_results:    \n",
    "    data = json.loads(segment)\n",
    "    \n",
    "    for phrase in data['recognizedPhrases']:\n",
    "        cx_finding=Finding(stt_findings_id, phrase.get('offsetInTicks'))\n",
    "        cx_finding.cx_channel_id=phrase.get('channel')\n",
    "            \n",
    "        nBests = phrase.get('nBest')\n",
    "        largest_confidence = nBests[0].get('confidence')\n",
    "        \n",
    "        for nBest in nBests:\n",
    "            if nBest.get('confidence') > largest_confidence:\n",
    "                largest_confidence = nBest.get('confidence')\n",
    "            \n",
    "        for nBest in nBests:\n",
    "            if nBest.get('confidence') == largest_confidence:\n",
    "                logging.info(f\"Best transctiption record : {nBest}\")\n",
    "                cx_finding.cx_transcript_confidence=nBest.get('confidence')\n",
    "                cx_finding.cx_raw_transcript=nBest.get('display')\n",
    "                    \n",
    "        findings.append(cx_finding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reteive the STT Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "dict_findings={}\n",
    "\n",
    "for finding in findings:\n",
    "    line = [ finding.cx_id, finding.cx_channel_id, finding.cx_feedback_theme, finding.cx_intent,  finding.cx_keywords, finding.cx_named_entity, finding.cx_transcript_offset, finding.cx_raw_transcript, finding.cx_transcript_confidence, finding.cx_sentiment  ]\n",
    "    lines.append(line)\n",
    "    dict_findings[str(finding.cx_transcript_offset)]=finding\n",
    "\n",
    "for key, finding in dict_findings.items():  \n",
    "    print(key, finding.cx_channel_id, finding.cx_transcript_offset, finding.cx_feedback_theme, finding.cx_intent, finding.cx_keywords, finding.cx_raw_transcript, finding.cx_transcript_confidence, finding.cx_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, finding in dict_findings.items():  \n",
    "   finding.cx_keywords=\"\"\n",
    "   finding.cx_named_entity=\"\"\n",
    "    \n",
    "for key, finding in dict_findings.items():  \n",
    "    print(key, finding.cx_channel_id,  \"|\", finding.cx_named_entity, \"|\", finding.cx_transcript_offset,  \"|\", finding.cx_feedback_theme,  \"|\", finding.cx_intent, finding.cx_keywords,  \"|\", finding.cx_raw_transcript,  \"|\", finding.cx_transcript_confidence,  \"|\",  finding.cx_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with Azure Text Analytics Service Key\n",
    "sub_key_nlp = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Ocp-Apim-Subscription-Key': sub_key_nlp,\n",
    "        }\n",
    "\n",
    "# Request parameters   \n",
    "params = urllib.parse.urlencode ({\n",
    "    'showStats': 'false',\n",
    "    'loggingOptOut': 'false',\n",
    "    'opinionMining': 'true',\n",
    "    'stringIndexType': 'TextElement_v8',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentanalysis():\n",
    "    logging.info(\"Starting Text Sentiment Analysis ......\")\n",
    "        \n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('languageSrv2.cognitiveservices.azure.com')\n",
    "        conn.request(\"POST\", \"/text/analytics/v3.1/sentiment?%s\" % params, json.dumps(stt_documents), headers)\n",
    "    \n",
    "        response = conn.getresponse().read().decode('utf-8')\n",
    "        data = json.loads(response)\n",
    "        sentiments_results = data['documents']\n",
    "        # print(sentiments_results)\n",
    "        for sentiment in sentiments_results:\n",
    "            offset=str(sentiment[\"id\"])\n",
    "            confidence=sentiment[\"sentiment\"]\n",
    "            dict_findings.get(offset).cx_sentiment = confidence\n",
    "            print(dict_findings.get(offset).cx_transcript_offset, dict_findings.get(offset).cx_raw_transcript, dict_findings.get(offset).cx_transcript_confidence, dict_findings.get(offset).cx_sentiment)\n",
    "        \n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Counter\n",
    "stt_documents={}\n",
    "documents_list=[]\n",
    "id=1\n",
    "\n",
    "for finding in findings:\n",
    "    value_dict = {\n",
    "                'id': str(finding.cx_transcript_offset),\n",
    "                'language': 'en',\n",
    "                'text': finding.cx_raw_transcript\n",
    "              }\n",
    "    \n",
    "    documents_list.append(value_dict)\n",
    "    id += 1\n",
    "    \n",
    "    if id > 10 :\n",
    "        stt_documents[\"documents\"] = documents_list\n",
    "        ## Reset Counter\n",
    "        sentimentanalysis()\n",
    "        stt_documents={}\n",
    "        documents_list=[]\n",
    "        id = 1\n",
    "    \n",
    "stt_documents[\"documents\"] = documents_list\n",
    "sentimentanalysis()\n",
    "#pprint(stt_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Phrases Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_feedback_theme={}\n",
    "dict_feedback_theme[\"Advertisement\"] = ['information', 'question', 'account', 'program requirements', 'policies']\n",
    "dict_feedback_theme[\"Communication\"] = ['shipping','USPS', 'customer service', 'exchange','return']\n",
    "\n",
    "dict_cx_intent={}\n",
    "                \n",
    "dict_cx_intent[\"Shipping\"] = ['shipping', 'USPS', 'place order','product']\n",
    "dict_cx_intent[\"Verification\"] = ['phone', 'email', 'questions', 'policies']\n",
    "dict_cx_intent[\"Order\"] = ['order Number', 'shipping ETA', 'arrive','status']\n",
    "dict_cx_intent[\"Product\"] = ['availability','product']\n",
    "dict_cx_intent[\"Return\"] = ['return label', 'black box', 'price error', 'wrong price']\n",
    "dict_cx_intent[\"Documentation\"] = ['Invoice Copy','Packing List']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeintent(key_phrases_results):\n",
    "\n",
    "    for key_phrases in key_phrases_results:\n",
    "        offset=str(key_phrases[\"id\"])\n",
    "        key_phrases_list=key_phrases[\"keyPhrases\"]\n",
    "        \n",
    "        intent_hit = []\n",
    "        keywords_hit = []\n",
    "        theme_hit = []\n",
    "            \n",
    "            \n",
    "        if len(key_phrases_list) != 0: \n",
    "            #print(key_phrases_list) \n",
    "            intent_hit = []\n",
    "            keywords_hit = []\n",
    "            theme_hit = []\n",
    "            \n",
    "            for key_phrase in key_phrases_list:\n",
    "                # loop through dict_cx_intent\n",
    "                for key, keywords in dict_cx_intent.items():\n",
    "                    # loop through keywords list\n",
    "                    for keyword in keywords:\n",
    "                        matcher = SequenceMatcher(None, key_phrase.lower(), keyword.lower())\n",
    "                        # setup bar value as 85% similarity\n",
    "                        if (matcher.ratio() > 0.85) :\n",
    "                            #print (\"comparing %s with %s: similarity is %s\" % (key_phrase, keyword, matcher.ratio()))\n",
    "                        \n",
    "                            if key not in intent_hit :\n",
    "                               intent_hit.append(key)\n",
    "                        \n",
    "                            # loop through theme list\n",
    "                            for theme, values in dict_feedback_theme.items():\n",
    "                                for value in values:\n",
    "                                    #print (\"comparing %s with %s: %s\" % (value, key, key == value)) \n",
    "                            \n",
    "                                    if (value == key) : \n",
    "                                        if theme not in theme_hit :\n",
    "                                            theme_hit.append(theme)\n",
    "                        \n",
    "                            if key_phrase not in keywords_hit :\n",
    "                               keywords_hit.append(key_phrase)\n",
    "                \n",
    "                if (dict_findings.get(offset).cx_keywords.strip().find(key_phrase) == -1):\n",
    "                    # print(\"adding ...\" + key_phrase)\n",
    "                    dict_findings.get(offset).cx_keywords += key_phrase + \",\"       \n",
    "                        \n",
    "        for intent in intent_hit:        \n",
    "            if not dict_findings.get(offset).cx_intent.strip():\n",
    "                    dict_findings.get(offset).cx_intent += intent\n",
    "            else :\n",
    "                dict_findings.get(offset).cx_intent += \",\" + intent\n",
    "   \n",
    "        for theme in theme_hit:        \n",
    "            if not dict_findings.get(offset).cx_feedback_theme.strip():\n",
    "                dict_findings.get(offset).cx_feedback_theme += theme\n",
    "            else :\n",
    "                dict_findings.get(offset).cx_feedback_theme += \",\" + theme\n",
    "    \n",
    "        # for kw in keywords_hit:\n",
    "        #    if not dict_findings.get(offset).cx_keywords.strip():\n",
    "        #        dict_findings.get(offset).cx_keywords += kw       \n",
    "        #    else :\n",
    "        #        dict_findings.get(offset).cx_keywords += \",\" + kw \n",
    "        \n",
    "        \n",
    "        print(dict_findings.get(offset).cx_transcript_offset, \"|\", dict_findings.get(offset).cx_keywords, \"|\", dict_findings.get(offset).cx_feedback_theme, \"|\", dict_findings.get(offset).cx_intent, \"|\", dict_findings.get(offset).cx_raw_transcript, \"|\", dict_findings.get(offset).cx_transcript_confidence, \"|\", dict_findings.get(offset).cx_sentiment)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyphraseextraction():\n",
    "    logging.info(\"Starting Key Phrase Extraction ......\")\n",
    "        \n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('languageSrv2.cognitiveservices.azure.com')\n",
    "        conn.request(\"POST\", \"/text/analytics/v3.1/keyPhrases?%s\" % params, json.dumps(stt_documents), headers)\n",
    "    \n",
    "        response = conn.getresponse().read().decode('utf-8')\n",
    "        data = json.loads(response)\n",
    "        key_phrases_results = data['documents']\n",
    "        #print(key_phrases_results)\n",
    "        analyzeintent(key_phrases_results)\n",
    "        \n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Counter\n",
    "stt_documents={}\n",
    "documents_list=[]\n",
    "id=1\n",
    "\n",
    "for finding in findings:\n",
    "    value_dict = {\n",
    "                'id': str(finding.cx_transcript_offset),\n",
    "                'language': 'en',\n",
    "                'text': finding.cx_raw_transcript\n",
    "              }\n",
    "    \n",
    "    documents_list.append(value_dict)\n",
    "    id += 1\n",
    "    \n",
    "    if id > 10 :\n",
    "        stt_documents[\"documents\"] = documents_list\n",
    "        ## Reset Counter\n",
    "        keyphraseextraction()\n",
    "        stt_documents={}\n",
    "        documents_list=[]\n",
    "        id = 1\n",
    "    \n",
    "stt_documents[\"documents\"] = documents_list\n",
    "keyphraseextraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractner(ner_results):\n",
    "\n",
    "    for ner in ner_results:\n",
    "        offset=str(ner[\"id\"])\n",
    "        entity_list=ner[\"entities\"]\n",
    "\n",
    "        for entity in entity_list: \n",
    "            # print(entity.get('category'))\n",
    "            category = entity.get('category')\n",
    "            \n",
    "            if (dict_findings.get(offset).cx_named_entity.strip().find(category) == -1):\n",
    "                # print(\"adding ...\" + category)\n",
    "                dict_findings.get(offset).cx_named_entity += category + \",\"    \n",
    "    \n",
    "        print(dict_findings.get(offset).cx_transcript_offset, \"|\", dict_findings.get(offset).cx_named_entity, \"|\", dict_findings.get(offset).cx_feedback_theme, \"|\", dict_findings.get(offset).cx_intent, \"|\", dict_findings.get(offset).cx_keywords, \"|\", dict_findings.get(offset).cx_raw_transcript, \"|\", dict_findings.get(offset).cx_transcript_confidence, \"|\", dict_findings.get(offset).cx_sentiment)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nameentityrecognition():\n",
    "    logging.info(\"Starting Named Entity Recognition ......\")\n",
    "        \n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('languageSrv2.cognitiveservices.azure.com')\n",
    "        conn.request(\"POST\", \"/text/analytics/v3.1/entities/recognition/general?%s\" % params, json.dumps(stt_documents), headers)\n",
    "    \n",
    "        response = conn.getresponse().read().decode('utf-8')\n",
    "        data = json.loads(response)\n",
    "        ner_results = data['documents']\n",
    "        #print(ner_results)\n",
    "        extractner(ner_results)\n",
    "        \n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Counter\n",
    "stt_documents={}\n",
    "documents_list=[]\n",
    "id=1\n",
    "\n",
    "for finding in findings:\n",
    "    value_dict = {\n",
    "                'id': str(finding.cx_transcript_offset),\n",
    "                'language': 'en',\n",
    "                'text': finding.cx_raw_transcript\n",
    "              }\n",
    "    \n",
    "    documents_list.append(value_dict)\n",
    "    id += 1\n",
    "    \n",
    "    if id > 5 :\n",
    "        stt_documents[\"documents\"] = documents_list\n",
    "        ## Reset Counter\n",
    "        nameentityrecognition()\n",
    "        stt_documents={}\n",
    "        documents_list=[]\n",
    "        id = 1\n",
    "    \n",
    "stt_documents[\"documents\"] = documents_list\n",
    "nameentityrecognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the Key Fidning Results to Local CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_lines = []\n",
    "\n",
    "for key, finding in dict_findings.items():  \n",
    "    csv_line = [ finding.cx_id, finding.cx_feedback_theme, finding.cx_intent,  finding.cx_keywords, finding.cx_named_entity, finding.cx_transcript_offset, finding.cx_raw_transcript, finding.cx_transcript_confidence, finding.cx_sentiment  ]\n",
    "    csv_lines.append(csv_line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt_findings_csv=\"123456789findings1.csv\"\n",
    "\n",
    "header=['cx_id','cx_feedback_theme', 'cx_intent','cx_keywords','cx_named_entity','cx_transcript_offset','cx_raw_transcript','cx_transcript_confidence','cx_sentiment']\n",
    "\n",
    "with open(stt_findings_csv, 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    writer.writerows(csv_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the Key Fidning Results to Azure Blob Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# replace with Azure Blob Storage name and folder\n",
    "blob_store_account_name = \"\"\n",
    "audio_container_name = ''\n",
    "finding_container_name = ''\n",
    "filefolder=\"\"\n",
    "## Replace with Azure Blob Account Key\n",
    "blob_storage_key = \"\"\n",
    "\n",
    "block_blob_service = BlockBlobService(account_name=blob_store_account_name, account_key=blob_storage_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with findings file name\n",
    "stt_findings_csv=\"\"\n",
    "\n",
    "header=['cx_id','cx_feedback_theme', 'cx_intent','cx_keywords','cx_named_entity','cx_transcript_offset','cx_raw_transcript','cx_transcript_confidence','cx_sentiment']\n",
    "\n",
    "with open(stt_findings_csv, 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    writer.writerows(csv_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path_to_file = os.path.join(os.getcwd(), stt_findings_csv)\n",
    "print(full_path_to_file)\n",
    "\n",
    "block_blob_service.create_blob_from_path(finding_container_name, stt_findings_csv, full_path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a csv file via pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = io.StringIO()\n",
    "df = pd.DataFrame (csv_lines , columns = header)\n",
    "#print(df)\n",
    "output = df.to_csv (index_label=\"idx\", encoding = \"utf-8\")\n",
    "print(output)\n",
    "\n",
    "block_blob_service.create_blob_from_text(finding_container_name, stt_findings_csv, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading STT analytic finding dataset into a Pandas data frame, visualizing and exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with Azure SQL DB Credential \n",
    "server_name = ''\n",
    "database_name   = ''\n",
    "userid        = ''\n",
    "password      = ''\n",
    "db_driver     = 'ODBC Driver 13 for SQL Server'\n",
    "\n",
    "#Driver={ODBC Driver 13 for SQL Server};Server=tcp:jnjsstdw.database.windows.net,1433;Database=jnjsttfinding;Uid=jnjsstadmin@jnjsstdw;Pwd={your_password_here};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the SQL DW Connection string\n",
    "driver = 'DRIVER={' + db_driver + '}'\n",
    "server = 'SERVER=' + server_name \n",
    "database = 'DATABASE=' + database_name\n",
    "uid = 'UID=' + userid \n",
    "pwd = 'PWD=' + password\n",
    "connection_string = ';'.join([driver,server,database,uid,pwd, 'Encrypt=yes;TrustServerCertificate=no'])\n",
    "print(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Database. Autocommit needs to turned on for DDL statements\n",
    "conn = pyodbc.connect(connection_string)\n",
    "conn.autocommit=True\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report number of rows and columns in table jnjcx.sstfindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = pd.read_sql('''SELECT SUM(rows) FROM sys.partitions WHERE object_id = OBJECT_ID('jnjcx.sst_findings')''', conn)\n",
    "print ('Total number of rows = %d' % nrows.iloc[0,0])\n",
    "\n",
    "ncols = pd.read_sql('''SELECT count(*) FROM information_schema.columns WHERE table_name = ('sst_findings') AND \n",
    "table_schema = ('jnjcx')''', conn)\n",
    "print ('Total number of columns = %d' % ncols.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset into Pandas DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql('''select top 10000 * from jnjcx.sst_findings t ''', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['cx_transcript_confidence'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.boxplot(column='cx_transcript_confidence',return_type='dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "df1['cx_transcript_confidence'].plot(ax=ax1,kind='kde', style='b-')\n",
    "df1['cx_transcript_confidence'].hist(ax=ax2, bins=100, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dist_bins = [0, 0.2, .4, 0.6, 0.8, 1]\n",
    "df1['cx_transcript_confidence']\n",
    "conf_dist_bin_id = pd.cut(df1['cx_transcript_confidence'], conf_dist_bins)\n",
    "conf_dist_bin_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(conf_dist_bin_id).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(conf_dist_bin_id).value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(conf_dist_bin_id).value_counts().plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql('''select top 10000 * from jnjcx.sst_findings t where t.cx_id='736021005637009' ''', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df1['cx_transcript_offset'], df1['cx_transcript_confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data for a specific audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "        SELECT *\n",
    "        FROM jnjcx.sst_findings t\n",
    "        where t.cx_id='736021005637009'\n",
    "        order by t.cx_transcript_offset\n",
    "        '''\n",
    "\n",
    "pd.read_sql(query, conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
